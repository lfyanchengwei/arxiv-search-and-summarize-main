#!/usr/bin/env python3
"""
ç”ŸæˆGitHub Actionsåˆ†ææŠ¥å‘Š
"""

import os
import json
import csv
import argparse
from datetime import datetime
from pathlib import Path


def read_csv_results(output_dir):
    """è¯»å–CSVåˆ†æç»“æœ"""
    csv_files = list(Path(output_dir).glob("enhanced_papers_analysis_*.csv"))
    if not csv_files:
        return None, None
    
    # è¯»å–æœ€æ–°çš„åˆ†ææ–‡ä»¶
    latest_csv = max(csv_files, key=os.path.getctime)
    
    papers = []
    with open(latest_csv, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        papers = list(reader)
    
    return papers, latest_csv


def read_summary_results(output_dir):
    """è¯»å–ç»Ÿè®¡æ‘˜è¦ç»“æœ"""
    summary_files = list(Path(output_dir).glob("enhanced_papers_summary_*.csv"))
    if not summary_files:
        return None
    
    latest_summary = max(summary_files, key=os.path.getctime)
    
    summary_data = {}
    current_section = None
    
    with open(latest_summary, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        for row in reader:
            if not row:
                continue
            if row[0].startswith("==="):
                current_section = row[0].strip("= ")
                summary_data[current_section] = []
            elif current_section and len(row) >= 2:
                summary_data[current_section].append(row)
    
    return summary_data


def read_run_info(output_dir):
    """è¯»å–è¿è¡Œä¿¡æ¯"""
    run_info_path = Path(output_dir) / "run_info.json"
    if run_info_path.exists():
        with open(run_info_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None


def generate_markdown_report(papers, summary_data, run_info, output_dir):
    """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
    
    report_lines = []
    
    # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
    report_lines.append("# ğŸ“Š å­¦æœ¯è®ºæ–‡åˆ†ææŠ¥å‘Š")
    report_lines.append("")
    report_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if run_info:
        config = run_info.get('config', {})
        report_lines.append(f"**åˆ†ææ—¶é—´èŒƒå›´**: {config.get('start_date')} åˆ° {config.get('end_date')}")
        report_lines.append(f"**æ£€ç´¢å…³é”®è¯**: {', '.join(config.get('search_keywords', []))}")
        report_lines.append(f"**ç ”ç©¶é¢†åŸŸ**: {', '.join(config.get('research_categories', []))}")
        report_lines.append(f"**æœ€å¤§è®ºæ–‡æ•°**: {config.get('max_papers')}")
        
        if run_info.get('preset_used'):
            report_lines.append(f"**ä½¿ç”¨é¢„è®¾**: {run_info['preset_used']}")
    
    report_lines.append("")
    
    # æ€»ä½“ç»Ÿè®¡
    if papers:
        report_lines.append("## ğŸ“ˆ æ€»ä½“ç»Ÿè®¡")
        report_lines.append("")
        report_lines.append(f"- **æ€»è®ºæ–‡æ•°é‡**: {len(papers)}")
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦å’Œåˆ›æ–°æ€§
        confidences = [float(p.get('Classification_Confidence', 0)) for p in papers if p.get('Classification_Confidence')]
        novelty_scores = [int(p.get('Novelty_Score', 0)) for p in papers if p.get('Novelty_Score')]
        
        if confidences:
            avg_confidence = sum(confidences) / len(confidences)
            report_lines.append(f"- **å¹³å‡åˆ†ç±»ç½®ä¿¡åº¦**: {avg_confidence:.2f}")
        
        if novelty_scores:
            avg_novelty = sum(novelty_scores) / len(novelty_scores)
            high_novelty_count = sum(1 for score in novelty_scores if score >= 4)
            report_lines.append(f"- **å¹³å‡åˆ›æ–°æ€§è¯„åˆ†**: {avg_novelty:.2f}")
            report_lines.append(f"- **é«˜åˆ›æ–°æ€§è®ºæ–‡æ•°é‡** (è¯„åˆ†â‰¥4): {high_novelty_count}")
        
        report_lines.append("")
    
    # ä»»åŠ¡åˆ†ç±»åˆ†å¸ƒ
    if summary_data and "ä»»åŠ¡ç±»åˆ«åˆ†å¸ƒ" in summary_data:
        report_lines.append("## ğŸ¯ ä»»åŠ¡åˆ†ç±»åˆ†å¸ƒ")
        report_lines.append("")
        report_lines.append("| ä»»åŠ¡ç±»åˆ« | è®ºæ–‡æ•°é‡ | å æ¯” |")
        report_lines.append("|---------|---------|------|")
        
        for row in summary_data["ä»»åŠ¡ç±»åˆ«åˆ†å¸ƒ"]:
            if len(row) >= 3 and row[0] != "Task_Category":
                report_lines.append(f"| {row[0]} | {row[1]} | {row[2]} |")
        
        report_lines.append("")
    
    # ç ”ç©¶é¢†åŸŸåˆ†å¸ƒ
    if summary_data and "ç ”ç©¶é¢†åŸŸåˆ†å¸ƒ" in summary_data:
        report_lines.append("## ğŸ”¬ ç ”ç©¶é¢†åŸŸåˆ†å¸ƒ")
        report_lines.append("")
        report_lines.append("| ç ”ç©¶é¢†åŸŸ | è®ºæ–‡æ•°é‡ | å æ¯” |")
        report_lines.append("|---------|---------|------|")
        
        for row in summary_data["ç ”ç©¶é¢†åŸŸåˆ†å¸ƒ"]:
            if len(row) >= 3 and row[0] != "Research_Field":
                report_lines.append(f"| {row[0]} | {row[1]} | {row[2]} |")
        
        report_lines.append("")
    
    # ä¸»è¦æœºæ„
    if summary_data and "ä¸»è¦æœºæ„åˆ†å¸ƒ" in summary_data:
        report_lines.append("## ğŸ›ï¸ ä¸»è¦æœºæ„åˆ†å¸ƒ")
        report_lines.append("")
        report_lines.append("| æœºæ„ | è®ºæ–‡æ•°é‡ |")
        report_lines.append("|------|---------|")
        
        for row in summary_data["ä¸»è¦æœºæ„åˆ†å¸ƒ"][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            if len(row) >= 2 and row[0] != "Institution":
                report_lines.append(f"| {row[0]} | {row[1]} |")
        
        report_lines.append("")
    
    # é«˜åˆ›æ–°æ€§è®ºæ–‡
    if papers:
        high_novelty_papers = [p for p in papers if p.get('Novelty_Score') and int(p['Novelty_Score']) >= 4]
        if high_novelty_papers:
            report_lines.append("## ğŸŒŸ é«˜åˆ›æ–°æ€§è®ºæ–‡ (è¯„åˆ†â‰¥4)")
            report_lines.append("")
            
            # æŒ‰åˆ›æ–°æ€§è¯„åˆ†æ’åº
            high_novelty_papers.sort(key=lambda x: int(x.get('Novelty_Score', 0)), reverse=True)
            
            for i, paper in enumerate(high_novelty_papers[:10], 1):  # æ˜¾ç¤ºå‰10ç¯‡
                title = paper.get('Title', 'æœªçŸ¥æ ‡é¢˜')
                score = paper.get('Novelty_Score', 'N/A')
                category = paper.get('Task_Category', 'æœªåˆ†ç±»')
                url = paper.get('ArXiv_URL', '')
                
                report_lines.append(f"### {i}. {title}")
                report_lines.append(f"- **åˆ›æ–°æ€§è¯„åˆ†**: {score}")
                report_lines.append(f"- **ä»»åŠ¡ç±»åˆ«**: {category}")
                if url:
                    report_lines.append(f"- **è®ºæ–‡é“¾æ¥**: [{url}]({url})")
                report_lines.append("")
    
    # æ–‡ä»¶ä¸‹è½½é“¾æ¥
    report_lines.append("## ğŸ“ è¯¦ç»†ç»“æœæ–‡ä»¶")
    report_lines.append("")
    report_lines.append("è¯·åœ¨GitHub Actionsçš„Artifactsä¸­ä¸‹è½½ä»¥ä¸‹æ–‡ä»¶ï¼š")
    report_lines.append("- ğŸ“Š è¯¦ç»†åˆ†æç»“æœ (CSV)")
    report_lines.append("- ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦ (CSV)")
    report_lines.append("- ğŸŒŸ é«˜åˆ›æ–°æ€§è®ºæ–‡ (CSV)")
    report_lines.append("")
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = Path(output_dir) / "analysis_summary.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    return report_path


def generate_json_summary(papers, summary_data, run_info, output_dir):
    """ç”ŸæˆJSONæ ¼å¼çš„æ‘˜è¦"""
    
    summary = {
        "timestamp": datetime.now().isoformat(),
        "run_info": run_info,
        "statistics": {
            "total_papers": len(papers) if papers else 0
        }
    }
    
    if papers:
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        confidences = [float(p.get('Classification_Confidence', 0)) for p in papers if p.get('Classification_Confidence')]
        novelty_scores = [int(p.get('Novelty_Score', 0)) for p in papers if p.get('Novelty_Score')]
        
        if confidences:
            summary["statistics"]["avg_confidence"] = sum(confidences) / len(confidences)
        
        if novelty_scores:
            summary["statistics"]["avg_novelty"] = sum(novelty_scores) / len(novelty_scores)
            summary["statistics"]["high_novelty_count"] = sum(1 for score in novelty_scores if score >= 4)
        
        # ä»»åŠ¡åˆ†ç±»ç»Ÿè®¡
        task_categories = {}
        for paper in papers:
            category = paper.get('Task_Category', 'æœªåˆ†ç±»')
            task_categories[category] = task_categories.get(category, 0) + 1
        
        summary["task_categories"] = task_categories
        
        # ç ”ç©¶é¢†åŸŸç»Ÿè®¡
        research_fields = {}
        for paper in papers:
            field = paper.get('Research_Field', 'æœªæ˜ç¡®è¯´æ˜')
            research_fields[field] = research_fields.get(field, 0) + 1
        
        summary["research_fields"] = research_fields
    
    # ä¿å­˜JSONæ‘˜è¦
    json_path = Path(output_dir) / "analysis_summary.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    return json_path


def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆåˆ†ææŠ¥å‘Š')
    parser.add_argument('--output_dir', type=str, default='output', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    print("ğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    
    # è¯»å–åˆ†æç»“æœ
    papers, csv_file = read_csv_results(args.output_dir)
    summary_data = read_summary_results(args.output_dir)
    run_info = read_run_info(args.output_dir)
    
    if not papers:
        print("âŒ æœªæ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶")
        return
    
    print(f"âœ… è¯»å–åˆ° {len(papers)} ç¯‡è®ºæ–‡çš„åˆ†æç»“æœ")
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_path = generate_markdown_report(papers, summary_data, run_info, args.output_dir)
    print(f"âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {md_path}")
    
    # ç”ŸæˆJSONæ‘˜è¦
    json_path = generate_json_summary(papers, summary_data, run_info, args.output_dir)
    print(f"âœ… JSONæ‘˜è¦å·²ç”Ÿæˆ: {json_path}")
    
    print("ğŸ‰ æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")


if __name__ == "__main__":
    main()